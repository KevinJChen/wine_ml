{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mltools as ml\n",
    "import sklearn.model_selection\n",
    "from sklearn import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 12)\n"
     ]
    }
   ],
   "source": [
    "combined_data = np.genfromtxt(\"combined_wine.csv\", delimiter = ',')\n",
    "combined_data = combined_data[~np.isnan(combined_data).any(axis=1)]\n",
    "Y_combined = combined_data[:,-1]\n",
    "X_combined = combined_data[:,0: -1]\n",
    "\n",
    "#shuffle data\n",
    "X, Y = ml.shuffleData(X_combined, Y_combined)\n",
    "print(X.shape)\n",
    "\n",
    "NUMBER_OF_FEATURES_WANTED = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "Read this article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016.51  4829.317  236.389  899.766 2315.829 1858.136 6252.796 1169.655\n",
      "  789.05  2021.708    7.068   93.812]\n",
      "(6497, 6)\n"
     ]
    }
   ],
   "source": [
    "#Unvariate Selection\n",
    "\n",
    "def unvariateSelection(number_of_features, precision = 3):\n",
    "    test = SelectKBest(score_func = f_classif, k = number_of_features)\n",
    "    fit = test.fit(X, Y)\n",
    "    # summarize scores\n",
    "    set_printoptions(precision=precision)\n",
    "    print(fit.scores_)\n",
    "    features = fit.transform(X)\n",
    "    # summarize selected features\n",
    "    print(features.shape)\n",
    "    return features\n",
    "\n",
    "X_unvariate = unvariateSelection(NUMBER_OF_FEATURES_WANTED)\n",
    "Xtr_unvariate, Xva_unvariate, Ytr_unvariate, Yva_unvariate = model_selection.train_test_split(X_unvariate, Y, test_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 6\n",
      "Selected Features: [ True  True  True False  True False False False  True  True False False]\n",
      "Feature Ranking: [1 1 1 3 1 6 5 2 1 1 4 7]\n",
      "(6497, 6)\n"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimantion\n",
    "\n",
    "def recursiveFeatureElimination(number_of_features, solver = 'lbfgs', max_iter = 2000):\n",
    "    model = LogisticRegression(solver=solver, max_iter = max_iter)\n",
    "    rfe = RFE(model, number_of_features)\n",
    "    fit = rfe.fit(X, Y)\n",
    "    features = fit.transform(X)\n",
    "    print(\"Num Features: %d\" % fit.n_features_)\n",
    "    print(\"Selected Features: %s\" % fit.support_)\n",
    "    print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "    print(features.shape)\n",
    "    return features\n",
    "\n",
    "X_recursive = recursiveFeatureElimination(NUMBER_OF_FEATURES_WANTED)\n",
    "Xtr_recursive, Xva_recursive, Ytr_recursive, Yva_recursive = model_selection.train_test_split(X_recursive, Y, test_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [9.536e-01 4.062e-02 4.826e-03 4.944e-04 3.467e-04 1.364e-04]\n",
      "[[-7.408e-03 -1.184e-03  4.869e-04  4.102e-02 -1.682e-04  2.305e-01\n",
      "   9.722e-01  1.772e-06 -6.555e-04 -7.043e-04 -5.452e-03 -5.327e-04]\n",
      " [-5.372e-03 -7.870e-04 -2.472e-04  1.863e-02  6.684e-05  9.726e-01\n",
      "  -2.314e-01  1.278e-06  6.480e-04  3.465e-04  2.879e-03  9.152e-03]\n",
      " [ 2.385e-02  9.047e-04  1.922e-03  9.952e-01  1.766e-04 -2.713e-02\n",
      "  -3.585e-02  4.608e-04 -6.911e-03 -1.936e-03 -8.260e-02 -8.792e-03]\n",
      " [ 7.134e-01  2.400e-02  2.403e-02 -7.050e-02  9.905e-03  1.081e-02\n",
      "   2.261e-03  1.439e-03 -2.761e-02  2.236e-02 -6.098e-01 -3.341e-01]\n",
      " [ 6.939e-01 -1.797e-02  5.098e-02  3.555e-02 -3.428e-03 -6.792e-04\n",
      "   7.397e-03 -1.705e-04 -3.771e-02  1.672e-02  5.857e-01  4.118e-01]\n",
      " [ 5.683e-02  4.207e-02 -3.093e-03  3.496e-02 -2.405e-03  6.097e-03\n",
      "   3.896e-05 -4.731e-04 -5.698e-03 -1.903e-02  5.270e-01 -8.459e-01]]\n",
      "(6497, 6)\n"
     ]
    }
   ],
   "source": [
    "#Principal Component Analysis\n",
    "# feature extraction\n",
    "def principalComponentAnalysis(number_of_features):\n",
    "    pca = PCA(n_components= number_of_features)\n",
    "    fit = pca.fit(X)\n",
    "    features = fit.transform(X)\n",
    "    # summarize components\n",
    "    print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "    print(fit.components_)\n",
    "    print(features.shape)\n",
    "    return features\n",
    "\n",
    "X_principal = principalComponentAnalysis(NUMBER_OF_FEATURES_WANTED)\n",
    "Xtr_principal, Xva_principal, Ytr_principal, Yva_principal = model_selection.train_test_split(X_principal, Y, test_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the fearure selected data for all three methods we can use\n",
    "random forest and neural netwrok classifiers to measure their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xva, Ytr, Yva = model_selection.train_test_split(X, Y, test_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Without Feature Selection\n",
      "Score: 0.945342571208622\n",
      "Score: 0.9249711427472105\n",
      "Training AUC: 0.9844127691839102\n",
      "Validation AUC: 0.9746768526557834\n",
      "\n",
      "With Unvariate Selection\n",
      "Score: 0.9361046959199384\n",
      "Score: 0.9324740284724894\n",
      "Training AUC: 0.9784093846745567\n",
      "Validation AUC: 0.9794940483636126\n",
      "\n",
      "With Recursive Feature Elimination\n",
      "Score: 0.9207082371054658\n",
      "Score: 0.9201616006156214\n",
      "Training AUC: 0.9359437953581043\n",
      "Validation AUC: 0.9279874512786385\n",
      "\n",
      "With Principal Component Analysis\n",
      "Score: 0.9260969976905312\n",
      "Score: 0.9228549442093112\n",
      "Training AUC: 0.9017553704626133\n",
      "Validation AUC: 0.8990895672492384\n"
     ]
    }
   ],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "def runRandomForest(Xtr, Ytr,Xva, Yva, number_of_features, number_estimators = 20, leafs = 2, depth = 1):\n",
    "    random_forest_classifier = sklearn.ensemble.RandomForestClassifier(n_estimators=number_estimators, max_features = number_of_features, min_samples_leaf=leafs, max_depth=depth, oob_score=True) \n",
    "    # better: n_estimator = 50 (increase complexity, variance), \n",
    "    # max_depth=2/3 (increase complexity, variance), max feature is good, \n",
    "    # adjust from 7->3 (reduce complexity, variance, increase bias)\n",
    "    classf = random_forest_classifier.fit(Xtr, Ytr)\n",
    "    print(\"Score:\", random_forest_classifier.score(Xtr, Ytr))\n",
    "    print(\"Score:\", random_forest_classifier.score(Xva, Yva))\n",
    "\n",
    "    #Article about Area Under the Curve (AUC) https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "    Ytr_score = classf.predict_proba(Xtr).T[1]\n",
    "    Yva_score = classf.predict_proba(Xva).T[1]\n",
    "    print(\"Training AUC: {}\".format(roc_auc_score(Ytr, Ytr_score)))\n",
    "    print(\"Validation AUC: {}\".format( roc_auc_score(Yva, Yva_score)))\n",
    "\n",
    "print(\"Base Without Feature Selection\")\n",
    "runRandomForest(Xtr, Ytr, Xva, Yva, X.shape[1])\n",
    "print(\"\\nWith Unvariate Selection\")\n",
    "runRandomForest(Xtr_unvariate, Ytr_unvariate, Xva_unvariate, Yva_unvariate, NUMBER_OF_FEATURES_WANTED)\n",
    "print(\"\\nWith Recursive Feature Elimination\")\n",
    "runRandomForest(Xtr_recursive , Ytr_recursive , Xva_recursive , Yva_recursive, NUMBER_OF_FEATURES_WANTED)\n",
    "print(\"\\nWith Principal Component Analysis\")\n",
    "runRandomForest(Xtr_principal, Ytr_principal, Xva_principal, Yva_principal, NUMBER_OF_FEATURES_WANTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Without Feature Selection\n",
      "Score: 0.985373364126251\n",
      "Score: 0.9774913428241632\n",
      "Training AUC: 0.9969379621408991\n",
      "Validation AUC: 0.9907784636815541\n",
      "\n",
      "With Unvariate Selection\n",
      "Score: 0.9799846035411856\n",
      "Score: 0.9715275105809927\n",
      "Training AUC: 0.9963398255978915\n",
      "Validation AUC: 0.993097702903518\n",
      "\n",
      "With Recursive Feature Elimination\n",
      "Score: 0.970746728252502\n",
      "Score: 0.9630627164293959\n",
      "Training AUC: 0.9922698125461068\n",
      "Validation AUC: 0.9852536512876449\n",
      "\n",
      "With Principal Component Analysis\n",
      "Score: 0.9522709776751347\n",
      "Score: 0.9470950365525203\n",
      "Training AUC: 0.9895396566182637\n",
      "Validation AUC: 0.9743306598085669\n"
     ]
    }
   ],
   "source": [
    "#NEURAL NETWORK\n",
    "\n",
    "def runNeuralNetwork(Xtr, Ytr,Xva, Yva, hidden_layers = (8, 8, 8), activation_func = 'relu', solver= 'adam', max_iter = 1000):\n",
    "    mlp_classifier = MLPClassifier(hidden_layer_sizes = hidden_layers, activation = activation_func, solver = solver, max_iter = max_iter)\n",
    "    classf = mlp_classifier.fit(Xtr, Ytr)\n",
    "    print(\"Score:\", mlp_classifier.score(Xtr, Ytr))\n",
    "    print(\"Score:\", mlp_classifier.score(Xva, Yva))\n",
    "\n",
    "    Ytr_score = classf.predict_proba(Xtr).T[1]\n",
    "    Yva_score = classf.predict_proba(Xva).T[1]\n",
    "    print(\"Training AUC: {}\".format(roc_auc_score(Ytr, Ytr_score)))\n",
    "    print(\"Validation AUC: {}\".format( roc_auc_score(Yva, Yva_score)))\n",
    "\n",
    "print(\"Base Without Feature Selection\")\n",
    "runNeuralNetwork(Xtr, Ytr, Xva, Yva)\n",
    "print(\"\\nWith Unvariate Selection\")\n",
    "runNeuralNetwork(Xtr_unvariate, Ytr_unvariate, Xva_unvariate, Yva_unvariate)\n",
    "print(\"\\nWith Recursive Feature Elimination\")\n",
    "runNeuralNetwork(Xtr_recursive , Ytr_recursive , Xva_recursive , Yva_recursive )\n",
    "print(\"\\nWith Principal Component Analysis\")\n",
    "runNeuralNetwork(Xtr_principal, Ytr_principal, Xva_principal, Yva_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Without Feature Selection\n",
      "Score: 1.0\n",
      "Score: 0.9763370527125818\n",
      "Training AUC: 1.0\n",
      "Validation AUC: 0.9691416847153711\n",
      "\n",
      "With Unvariate Selection\n",
      "Score: 1.0\n",
      "Score: 0.9759522893420547\n",
      "Training AUC: 1.0\n",
      "Validation AUC: 0.9665610599366561\n",
      "\n",
      "With Recursive Feature Elimination\n",
      "Score: 1.0\n",
      "Score: 0.9653712966525587\n",
      "Training AUC: 1.0\n",
      "Validation AUC: 0.9513277728020975\n",
      "\n",
      "With Principal Component Analysis\n",
      "Score: 1.0\n",
      "Score: 0.9392073874567142\n",
      "Training AUC: 1.0\n",
      "Validation AUC: 0.9194067877174635\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "\n",
    "def runDecisionTree(Xtr, Ytr,Xva, Yva, criterion = 'entropy', depth = 10):\n",
    "    dt_classifier = DecisionTreeClassifier(criterion = criterion, max_depth = depth)\n",
    "    classf = dt_classifier.fit(Xtr, Ytr)\n",
    "    print(\"Score:\", dt_classifier.score(Xtr, Ytr))\n",
    "    print(\"Score:\", dt_classifier.score(Xva, Yva))\n",
    "\n",
    "    Ytr_score = classf.predict_proba(Xtr).T[1]\n",
    "    Yva_score = classf.predict_proba(Xva).T[1]\n",
    "    print(\"Training AUC: {}\".format(roc_auc_score(Ytr, Ytr_score)))\n",
    "    print(\"Validation AUC: {}\".format( roc_auc_score(Yva, Yva_score)))\n",
    "\n",
    "print(\"Base Without Feature Selection\")\n",
    "runDecisionTree(Xtr, Ytr, Xva, Yva)\n",
    "print(\"\\nWith Unvariate Selection\")\n",
    "runDecisionTree(Xtr_unvariate, Ytr_unvariate, Xva_unvariate, Yva_unvariate)\n",
    "print(\"\\nWith Recursive Feature Elimination\")\n",
    "runDecisionTree(Xtr_recursive , Ytr_recursive , Xva_recursive , Yva_recursive )\n",
    "print(\"\\nWith Principal Component Analysis\")\n",
    "runDecisionTree(Xtr_principal, Ytr_principal, Xva_principal, Yva_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
